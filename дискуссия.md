Хочу поговорить о системном дизайне фреймворков и интерфейсов. Я хочу сделать фреймворк на Python, с помощью которого можно было бы проектировать сложные пайплайны с большим количеством обратных связей внутри. Вот представь, что мы хотим сделать пайплайн, в котором мы хотим симулировать небольшого робота, разными частями которого управляют нейронные сети, обучающиеся с помощью RL и контроллеры типа MPC, а ещё там есть модули, отвечающие за path planning и всё это работает на симуляционных датчиков, которые работают с задержками и шумом и при этом при всём мы хотим это визуализировать, а ещё мы хотим визуализировать, строить какие-то дашборды онлайн, а ещё хотим, оптимизировать все гиперпараметры наших моделей, и чтобы всё это ещё и быстро работало, и чтобы у меня не ушло сто лет на дебаг, потому что это колоссально тяжело оформить в единый проект, чтобы всё было читаемо, конфигурируемо и отказоустойчиво.

И давай теперь подумаем, как индуктивно к этому подойти.

Допустим я пишу просто main loop типа while True: ... - тогда код главного цикла будет очень ветвистым, где порядок запуска логических частей типа контроллеров и оптимизаторов будет определяться некоторым "состоянием", которое мы будем прокидывать сквозь пайплайн. Даже простейшие RL пайплайны "один алгоритм - одна система" это много строк с костылями типа "observation, observation_last", которые появляются, потому что нам надо пополнять дата буфер, но иногда нам нужен +-1 семпл или просто запоминать последнее состояние, и это всё плодит кучу абсолютно нечитаемого кода. Я молчу о том, что если я хочу что-то добавить (например визуализатор), мне надо будет открыть main loop на полторы-две тысячи строк и ковырять его и видоизменять, что влечёт за собой нужду в том, чтобы делать новую ветку в гите или форкать репозиторий (банальный пример - PPO имплементация в Stable Baselines, в которой из коробки нет PPO, работающего на рекуррентных нейросетях, поскольку рекуррентность влечёт небольшое изменение data flow, что абсолютно невозможно сделать относительно бесшовно)

Ну хорошо, как это можно разрешить? Надо подумать, что можно сделать автоматически. То есть, задуматься над тем, КАКУЮ ИНФОРМАЦИЮ можно утилизировать для того, чтобы какой-то бойлерплейт сделать ЗА юзера. Ну понятно, что если у нас в задаче можно чётко выделить domain сущности (их можно назвать нодами), то мы можем построить schedule выполнения сущностей автоматически, зная зависимости состояния нод от состояния других нод и таким образом избавиться от main loop, автоматически построив DAG. Проблема заключается в том, что DAG по определению не включает в себя петли, потому что он acyclic, lol
Поэтому если у нас в одной естественной итерации графа одна и та же нода должна сработать несколько раз (банальный например пополнение дата-буфера. Я могу захотеть взять семпл после обновления action у актора и потом взять значение Q функции после применения action во время наблюдения нового состояния), то нам придётся расклеивать эти петли и делать промежуточные итерации графа, что является, строго говоря, костылём и начинает затруднять и усложнять имплементацию. Получается ситуация "шило на мыло".

Кажется, что эту парадигму можно ещё сильнее продвинуть, и надо подумать, как именно. Причём это должен быть некий понятный, логически обоснованный шаг.

Я вот чего не понимаю с формальной точки зрения: мы когда пишем классы с объявлением переменных внутри, то с точки зрения computer science мы используем некоторый DSL - декларативный API к ядру фреймворка. А если это так, то получается, что нам надо идти не от частного к общему, описывая возможные краевые случаи и делая костыли, а как-то уметь взять такой набор нод, их реакций, декораторов и переменных в полях и либо "скомпилировать" либо заругаться и сказать, где ошибка. Но я вообще не понимаю, как это сделать, если честно.

-------

Это ключевой вопрос. Давай разберём формально.

## Что такое "программа" в этом DSL?

С математической точки зрения, набор нод с реакциями определяет **систему уравнений с темпоральными зависимостями**:

```
x[t+1] = f(x[t], u[t])           // дискретная динамика
ẋ(t) = g(x(t), u(t))             // непрерывная динамика
y[t] = h(x[t])                   // выходы
u[t] = k(y'[t], y'[t-1], ...)    // контроллер читает чужие выходы
```

Где `t` — это либо дискретное время (microstep), либо непрерывное.

## Что значит "корректная программа"?

Есть несколько уровней корректности:

### 1. **Структурная корректность**
- Все Input подключены к какому-то Output
- Типы совместимы (`Output[float] → Input[float]`)
- Нет использования необъявленных переменных

Это тривиально — обычный type checking.

### 2. **Каузальная корректность (Causality)**

Вот где сложнее. Вопрос: можно ли вычислить все значения в порядке, который не требует знания будущего?

Для **чисто дискретных** систем это задача о **топологической сортировке** графа зависимостей. Если граф DAG — можно. Если есть циклы — нужен особый подход.

### 3. **Циклы и сходимость**

Циклы бывают двух типов:

**Тип A: Алгебраические циклы (instantaneous feedback)**
```
y = f(x)
x = g(y)    // В тот же момент времени!
```

Это **система алгебраических уравнений**. Решается итеративно:
```
x_0 = initial
y_1 = f(x_0)
x_1 = g(y_1)
y_2 = f(x_1)
...
```

Вопрос: **сойдётся ли?** Ответ зависит от свойств `f` и `g`.

**Теорема (Kleene/Tarski):** Если `f` и `g` монотонны на полурешётке с конечной высотой, итерации сходятся за конечное число шагов.

Это именно то, что делает Regelum с semilattice modes:
- `BusMode` — append-only, высота = ∞, но с bounds
- `DeltaSumMode` — сумма, монотонна
- `LastWriterWinsMode` — не монотонна! Требует особой обработки

**Тип B: Временные циклы (delay feedback)**
```
x[t+1] = f(x[t], y[t])
y[t] = g(x[t])
```

Это **не** алгебраический цикл — это просто динамическая система. Вычисляется последовательно по времени, проблем нет.

---

## Формальная модель: Tagged Dataflow

Теперь к формализации. Используем модель из Ptolemy II / Lingua Franca:

**Определение 1: Сигнал**
```
Signal = Time → Maybe Value
```
Сигнал — частичная функция от времени в значения.

**Определение 2: Реакция**
```
Reaction = (Input Signals, Current State) → (Output Signals, New State)
```

**Определение 3: Tag**
```
Tag = (t: ℝ⁺, μ: ℕ)   // (время, microstep)
```
С лексикографическим порядком.

**Определение 4: Event**
```
Event = (Tag, ReactionId, Payload)
```

**Определение 5: Программа корректна, если:**

1. **Well-typed:** типы на границах совместимы
2. **Causal:** для каждой реакции `r`, если она читает сигнал `s` в tag `τ`, то `s` определён в `τ` или раньше
3. **Terminating per tag:** для каждого `t`, последовательность microsteps конечна
4. **Deterministic:** результат не зависит от порядка выполнения реакций в одном microstep

---

## Как проверить корректность?

### Шаг 1: Построить граф зависимостей

```python
@dataclass
class DependencyEdge:
    source: ReactionId
    target: ReactionId
    kind: Literal["same_time", "delayed"]
    via_variable: VariableId | None
```

Граф строится из:
- `reaction.writes → reaction.triggers` (если запись триггерит реакцию)
- Явные `same_time` связи
- Delay connections (для delayed — edge в следующий tag)

### Шаг 2: Найти SCC (strongly connected components)

Алгоритм Tarjan. Каждый SCC — потенциальный алгебраический цикл.

### Шаг 3: Для каждого нетривиального SCC проверить:

**A) Все переменные в цикле имеют monotone mode?**
```python
for var in scc.written_variables:
    if not var.mode.is_inflationary():
        raise CausalityError("non-monotone variable in cycle")
```

**B) Height bound конечен?**
```python
total_height = sum(var.mode.height_bound() for var in scc.written_variables)
if total_height == ∞:
    raise CausalityError("unbounded cycle")
```

**C) Нет LWW с несколькими writers без tie-breaker?**
```python
for var in scc.written_variables:
    writers = reactions_writing_to(var)
    if isinstance(var.mode, LWW) and len(writers) > 1:
        if var.mode.tie_breaker is None:
            raise CausalityError("LWW needs tie-breaker")
```

### Шаг 4: Проверить timing consistency

Для multi-rate систем:
```python
for connection in connections:
    source_period = connection.source.clock.period
    target_period = connection.target.clock.period
    
    # Nyquist: источник должен быть не медленнее получателя
    # или нужен explicit downsampling
    if source_period > target_period * 2:
        warn("Nyquist violation: add anti-aliasing filter")
```

---

## Алгоритм "компиляции"

```
Compile(nodes, connections):
    1. Parse: extract Variables, Reactions from class definitions
    2. TypeCheck: verify Input/Output type compatibility
    3. BuildGraph: create dependency graph from writes/triggers
    4. FindSCCs: Tarjan's algorithm
    5. ValidateSCCs: check monotonicity, height bounds
    6. ComputeSchedule: topological sort of SCCs
    7. InferClocks: propagate clock domains
    8. AllocateState: initialize variables, buffers
    9. Return: ExecutableProgram
```

Если любой шаг fail — возвращаем **структурированную ошибку** с указанием:
- Где проблема (какие ноды/реакции/переменные)
- Почему это проблема
- Как исправить (fix hints)

---

## Пример: как это должно выглядеть

```python
class BadExample(Node):
    x = State(0.0)
    
    @reaction(reads=["x"], writes=["x"])
    def cycle(self, x):
        return x + 1  # Цикл без bounds!

# При попытке создать pipeline:
pipeline = Pipeline(BadExample())

# → CompileError: Zero-delay cycle detected in reaction 'BadExample.cycle'
#   
#   The variable 'x' is written and read by the same reaction within
#   the same logical time, creating an instantaneous feedback loop.
#   
#   The current mode (implicit LWW) does not guarantee convergence.
#   
#   Fixes:
#     1. Add a delay: @reaction(..., delay=0.01)
#     2. Use a monotone mode: x = State(0.0, mode=DeltaSum())
#     3. Add explicit bounds: x = State(0.0, mode=DeltaSum(height=10))
```

---

## Ключевой insight

Вопрос "как скомпилировать DSL" сводится к вопросу:

> **Можно ли из декларативного описания вывести детерминированный schedule выполнения?**

Ответ: **да, если выполнены causality constraints.**

А causality constraints — это:
1. Нет зависимости от будущего (тривиально для discrete time)
2. Все алгебраические циклы сходятся (semilattice + height bound)
3. Типы совместимы (стандартный type checking)

Это **статически проверяемо** до запуска.


Меня интересует, что я потерял или забыл? Есть ли ещё какие-то критерии корректности, которые нужно проверять, но были мной упущены? 


========================

**Restated Question / Переформулировка запроса**

* **EN.** *Given the formal view of the DSL (nodes as equations over time, SCC analysis, Tarski/Kleene fixed points, etc.), what correctness criteria am I still missing? What else should a “compiler” check besides typing, causality and convergence of algebraic cycles?*
* **RU.** *С учётом формализации DSL (ноды как уравнения во времени, SCC‑анализ, теорема Тарского/Клини, проверка причинности), какие критерии корректности я ещё упустил? Что ещё должен проверять «компилятор», помимо типизации, каузальности и сходимости алгебраических циклов?*

---

## 0. Что у тебя уже есть (и это правда «по науке»)

Ты по сути воспроизвёл классическую картину synchronous/dataflow‑миров:

* **Структурная корректность**: well‑typed, no unconnected inputs, нет ссылок на несуществующие переменные.
* **Каузальность**: зависимость только от прошлого/текущего, топосорт по DAG, SCC для циклов.
* **Алгебраические циклы → фикс‑пойнт**: монотонный оператор на конечной полу‑решётке → существует наименьшая неподвижная точка (Тарский/Клини).
* **Разделение instantaneous vs delayed циклов**: distinction A/B у тебя — это стандартное разделение “algebraic loop vs delayed feedback” в Simulink/синхронных языках.

Это уже почти то, что делают causality‑анализы в Lustre/Scade/Esterel и SR‑домен Ptolemy.²⁰ ⁰⁵ ⁰⁰ ([di.ens.fr][1])

Дальше — то, чего *не хватает*, если ориентироваться на опыт synchronous‑языков и dataflow‑анализа.

---

## 1. Constructive causality, а не только «есть фикс‑пойнт»

Сейчас ты проверяешь, что на цикл можно повесить **монотонный оператор** на конечной решётке → наименьшая неподвижная точка существует и итеративное вычисление сойдётся. Но в синхронных языках этого **мало**: различают

* **Просто детерминизм** (существует *какой‑то* однозначный результат),
* И **конструктивную каузальность** (есть “доказательство без угадывания” значения сигналов, в духе интуиционистской логики).⁰⁵ ⁰⁸ ⁰¹² ⁰³² ([ptolemy.berkeley.edu][2])

Пример из Esterel: программы, которые логически непротиворечивы, но требуют «догадаться», присутствует ли сигнал, а потом проверить, что догадка не даёт противоречия — такие считаются **неконструктивными** и компилятор их просто запрещает.⁰⁴ ⁰¹² ([cs.columbia.edu][3])

**Доп. критерий:**

> Для каждого алгебраического цикла должно существовать *конструктивное* вычисление фикс‑пойнта (например, трёхзначная логика с итерацией от ⊥ к определённым значениям), не использующее отрицание будущих значений.

Практически это выглядит как:

* Запускаешь фикс‑пойнт‑итерацию в трёхзначной логике (`⊥` / known‑0 / known‑1) как в анализе «constructive circuits» (Shiple et al.).⁰¹⁹ ([University of Kiel - Computer Science][4])
* Если в конце кое‑где остаётся `⊥` → модель **неконструктивна**, и её нужно запретить (или потребовать явных delay/guards).

Это строгое усиление твоих «монотонно + bounded height», и это прямо то, что сделали в Esterel/Scade, чтобы запретить «магические» программы.⁰⁵ ⁰¹² ([ptolemy.berkeley.edu][2])

---

## 2. Non‑Zeno / Termination per tag (чтобы не было бесконечных microstep’ов)

Ты уже написал условие:

> 3. Terminating per tag: для каждого `t` последовательность microsteps конечна.

Это по сути требование **отсутствия Zeno‑поведения** на уровне дискретных событий: бесконечного числа переходов в конечное время. В гибридных системах это одна из центральных проблем.⁰¹⁴ ⁰¹⁰ ⁰¹⁷ ([ptolemy.berkeley.edu][5])

Что добавить формально:

1. **Локальный Zeno‑детектор для реакций**:

   * Если реакция при срабатывании *всегда* порождает новый event в **том же `Tag`** и не уменьшает никакой потенциальной функции (ранг), это кандидат на Zeno‑цикл.
   * Можно ввести простую «структурную» метрику: число специфических “instant”‑transition’ов в пути не должно расти без bound.

2. **Глобальное правило для superdense time:**

   * Множество Tag’ов `(t, μ)` должно быть **хорошо упорядочено**: у тебя нет бесконечной строго возрастающей последовательности с фиксированным `t`. В терминах Lee/Nutaro: допускаем superdense time, но не допускаем *ω*‑цепочку microstep’ов на одном `t`.⁰² ⁰⁶ ⁰³³ ([OSTI][6])

**Статически** это полностью неразрешимо, но можно иметь довольно мощный **консервативный анализ**: реакция, которая в `Tag` не может никак породить событие с `t' > t`, и при этом стоит в цикле → флаг как потенциальный Zeno.

---

## 3. Конфликты при многократной записи (deterministic shared writes)

Ты уже упомянул LWW и tie‑breaker, но общая картина такая:

Если переменную `v` в одном `Tag` пишут несколько реакций, то для детерминизма нужно, чтобы:

1. Либо на `v` определена **коммутативно‑ассоциативная моноидальная операция** `⊕` с нейтральным элементом, и `v` в этом тике = `⊕` всех вкладов (сумма, max, логическое И/ИЛИ и т.п.),
2. Либо есть **тотальный порядок** на writers (tie‑breaker), и в DSL это явно видно пользователю,
3. Либо компилятор ругается: «несогласованные записи без определения политики объединения».

Это частный случай твоей проверки для LWW, но стоит вывести это как **общий критерий детерминизма**:

> Для каждого `(variable, Tag)` либо не более одного писателя, либо задан детерминированный комбайнер `⊕` (моноид) *или* порядок писателей.

В synchronous моделях это обычно зашито в MOC — например, SDF/КПН полагаются на FIFO‑очереди и отсутствие shared state; в SR/DE это часто явная политика «мержа сигналов».⁰⁵ ⁰²¹ ([ptolemy.berkeley.edu][2])

---

## 4. Инициализация и «отсутствие» (bottom vs absent)

Ты работаешь с `Signal = Time → Maybe Value`, но в проверках не прозвучала отдельная история про **инициализацию** и **bottom**:

* В синхронных языках есть различие между

  * `⊥` (неизвестно / non‑causal / неудачная попытка вычислить),
  * и *отсутствием* значения в такте (present/absent семантика).⁰⁰ ⁰²⁴ ([di.ens.fr][1])
* Цель causality‑анализа (Pouzet, Colaco):

  1. гарантировать, что переходная функция *не* возвращает `⊥` (bottom),
  2. и что все сигналы/стримы определены для каждого такта по своим часам.⁰⁰ ⁰²⁴ ([di.ens.fr][1])

**Критерий, который стоит добавить:**

* Каждая state‑переменная имеет **явный начальный элемент** решётки (обычно bottom/нейтральный элемент).
* Ни одна реакция не читает переменную в `Tag`, где она может быть только `⊥` (ещё не определена). Это чуть сильнее, чем просто «подключены все Inputs» — это уже *temporal initialization*.

Практически: нужен pass `InitCheck`, который для каждого `Variable` и каждого её “clock domain” убеждается, что перед первым чтением переменная либо инициализирована, либо у неё есть well‑defined default.

---

## 5. Liveness / deadlock / boundedness для dataflow‑подграфов

Ты сейчас смотришь на граф как на общую систему уравнений. Но если какие‑то куски твоей модели на самом деле **SDF‑подграфы** (actors + FIFO channels с фиксированными rate’ами), то для них есть мощные *дополнительные* критерии, которые даёт Synchronous Dataflow:

* **Consistency** → существует статический schedule, при котором потоки не нарушают balance equations.
* **Liveness + boundedness** → нет deadlock’а и все FIFO‑буферы ограничены по размеру.⁰¹⁵ ⁰²⁶ ⁰³⁰ ⁰³⁴ ([ptolemy.berkeley.edu][7])

Практически:

* Для SDF‑фрагментов (где известно, сколько токенов потребляется/производится) можно прогнать **SDF‑анализ**: решение balance‑уравнений, проверка на наличие статического schedule, оценка потребного размера буферов.
* Если SDF‑анализ даёт, что подграф не live или требуется бесконечный буфер → **ошибка или жёсткое предупреждение** ещё до runtime.

Это даёт тебе:

> Доп. уровень корректности: *не только* «можно вычислить значения», но и «это можно делать с конечной памятью и без deadlock’ов» в конкретном поддомене.

---

## 6. Гибридность (если ты реально хочешь поддерживать ẋ = g(x,u))

Ты в начале упоминал непрерывную динамику (`ẋ(t) = g(x,u)`). Для таких систем появляются ещё два формальных аспекта:

1. **Существование и единственность решения ODE** на каждом шаге (условия типа Липшица / ограничения шага интегрирования).⁰²¹ ([Semantic Scholar][8])
2. **Zeno‑поведение гибридной системы** (микс дискретных переходов и непрерывной эволюции) — Lee/Ames показывают, что Zeno в основном дискретная проблема, но непрерывная часть влияет на моменты переходов.⁰¹⁴ ⁰¹⁷ ⁰²⁹ ([ptolemy.berkeley.edu][5])

Формально ты можешь поступить прагматично:

* В DSL **CT‑субсистемы** оформить как отдельный тип ноды, для которой есть контракты с solver’ом (типа «локальная Lipschitz‑постоянная» и max step size).
* На уровне статанализа **только проверять**, что все «reset/guard» для этих нод не создают Zeno‑циклы без прогресса времени (смотри пункт 2).

Полностью формальный анализ гибридных автоматов дорогой, но даже консервативные проверки (без бесконечного числа переходов на одном `t`) сильно помогают.⁰²³  ([ep.liu.se][9])

---

## 7. Fairness / progress (микро‑лосс в модели исполнения)

То, что часто забывают:

* Даже при отсутствии мёртвых циклов на уровне зависимостей может быть проблема **starvation**: некоторые реакции никогда не получают шанса от scheduler’а.
* В synchronous/DE‑мира эта проблема решается *договором* о scheduler’е (round‑robin, приоритеты, fairness‑условия).

Формально:

> В определении корректности можно добавить: любая реакция, для которой бесконечно часто выполняются условия триггера, должна бесконечно часто выполняться (fair scheduling).

Это уже не совсем проверяется «раз и навсегда», но разумно зафиксировать как часть *семантики DSL* и использовать в доказательствах инвариантов.

---

## 8. Обновлённый «Compile» с учётом этих критериев

Если собрать всё в один формальный пайплайн, получится что‑то вроде:

```text
Compile(nodes, connections):
    0. Clock / activation analysis:
       - определить часы/домены времени для каждой переменной/реакции

    1. Structural type check:
       - совместимость типов; нет неинициализированных ссылок

    2. Build dependency graph (instantaneous vs delayed edges)

    3. SCC decomposition:
       - для алгебраических SCC отдельный анализ

    4. Algebraic-SCC analysis:
       (a) проверка монотонности + конечной высоты решёток
       (b) constructive-causality check (3-valued iteration)
       (c) проверка политики объединения записей (моноид ⊕ или tie-breaker)
       (d) оценка Zeno-риск (циклы с только instant edges и без уменьшения ранга)

    5. Schedule / director selection:
       - топосорт SCC, выбор DE/SR/BSP-директора для подграфов

    6. Dataflow subgraph detection:
       - для SDF-подграфов → SDF-анализ (consistency, boundedness, deadlock)

    7. Initialization check:
       - для каждой переменной: есть начальное значение или гарантированная запись до первого чтения

    8. Resource / liveness sanity:
       - хотя бы консервативные оценки очередей и числа microstep-ов на tag

    9. Code generation:
       - построить исполняемую структуру (schedule + runtime hooks)
```

Любой fail → структурированная ошибка с указанием SCC/переменной/реакции и, по возможности, **hint’ом**, как исправить (вставить delay, задать combine‑операцию, изменить mode, добавить init и т.п.).

---

## Сжатый список того, чего не хватало

1. **Constructive causality (Esterel‑стайл)**, а не только «фикс‑пойнт существует».⁰⁵ ⁰⁸ ⁰¹² ⁰³² ([ptolemy.berkeley.edu][2])
2. **Формализованный non‑Zeno / termination per tag** для superdense времени.⁰² ⁰⁶ ⁰¹⁴ ⁰¹⁷ ([OSTI][6])
3. **Детерминированное слияние множественных записей** (моноид / порядок / ошибка).⁰⁵ ([ptolemy.berkeley.edu][2])
4. **Инициализация против bottom’а**, явное различение «неизвестно» и «отсутствует», как в causality‑аналитике Lustre/Scade.⁰⁰ ⁰²⁴ ([di.ens.fr][1])
5. **Liveness + boundedness для SDF‑подграфов** (если они есть).⁰¹⁵ ⁰²⁶ ⁰³⁰ ⁰³⁴ ([ptolemy.berkeley.edu][7])
6. **Гибридная корректность** (существование/единственность ODE‑решений, отсутствие Zeno в гибридной части).⁰²¹ ⁰²³  ([Semantic Scholar][8])
7. **Fairness / прогресс** как часть семантики scheduler’а.

---

## Термины (кратко и в лоб)

* **Constructive causality** — не только логическая непротиворечивость, но и существование *конструктивного* алгоритма вычисления сигналов в каждом такте, без угадываний «присутствует/отсутствует».⁰⁵ ⁰⁴ ([ptolemy.berkeley.edu][2])
* **Zeno‑поведение** — бесконечное число дискретных переходов за конечный физический интервал времени (или бесконечно много microstep‑ов на одном `t`).⁰¹⁴ ⁰¹⁷ ([ptolemy.berkeley.edu][5])
* **SDF boundedness/liveness** — свойства Synchronous Dataflow‑графа: нет deadlock‑а, все очереди имеют конечный размер при «самотактировании».⁰¹⁵ ⁰³⁰ ([ptolemy.berkeley.edu][7])

---

## Ссылки (Vancouver) + короткий «claim → evidence → critique»

1. Pouzet M. *Causality analysis in synchronous languages.* Lecture notes; 2023. URL: di.ens.fr/~pouzet/.../causality.pdf — **Claim:** causality‑анализ нужен, чтобы (1) реакция не возвращала ⊥, (2) можно было построить schedule; **Evidence:** формальное описание анализа Lustre; **Critique:** фокус на stream‑языках. ([di.ens.fr][1])
2. Edwards SA. *Synchronous‑Reactive Models.* In: Ptolemaeus C, ed. *System Design, Modeling, and Simulation using Ptolemy II.* 2014. URL: ptolemy.berkeley.edu/... — **Claim:** различие между детерминированной и конструктивной семантикой SR‑моделей; **Evidence:** раздел “Sidebar: Causality in Synchronous Languages”; **Critique:** академичность, не конкретный DSL. ([ptolemy.berkeley.edu][2])
3. Benveniste A et al. *The synchronous languages 12 years later.* *Proc IEEE* 2003;91(1):64–83. URL: cs.columbia.edu/~sedwards/.../benveniste2003synchronous.pdf — **Claim:** Esterel базируется на constructive causality; **Evidence:** обсуждение проблемы каузальности и эволюции семантики; **Critique:** исторический обзор, мало практических API. ([cs.columbia.edu][10])
4. Edwards SA. *The Synchronous Language Esterel.* Columbia Univ. 2002. URL: cs.columbia.edu/.../esterel.pdf — **Claim:** вводит constructive causality и примеры программ, которые логически корректны, но неконструктивны; **Evidence:** разделы о causality examples; **Critique:** язык‑специфично. ([cs.columbia.edu][3])
5. Nutaro JJ. *Toward a Theory of Superdense Time in Simulation Models.* *ACM TOMACS* 2020;30(3). URL: dl.acm.org/doi/10.1145/3379489 — **Claim:** формализует superdense time и варианты устранения Zeno‑эффектов; **Evidence:** теория и примеры; **Critique:** ориентирован на DES/симуляцию. ([ACM Digital Library][11])
6. Lee EA. *The Operational Semantics of Hybrid Systems.* HSCC 2005 tutorial slides. URL: ptolemy.berkeley.edu/presentations/05/hybridSemantics_HSCC.pdf — **Claim:** Zeno — свойство дискретных событий; его можно почти полностью рассматривать в DE‑части; **Evidence:** слайды “Issue 5: Zeno Conditions”; **Critique:** обзорный формат. ([ptolemy.berkeley.edu][5])
7. Ames AD. *Sufficient Conditions for the Existence of Zeno Behavior.* CDC 2005. URL: ames.caltech.edu/AmesCDC2005.pdf — **Claim:** даёт условия существования Zeno‑поведения для класса гибридных систем; **Evidence:** формальные теоремы; **Critique:** специфичный класс систем. ([ames.caltech.edu][12])
8. Groote R de. *On the Analysis of Synchronous Dataflow Graphs.* PhD Thesis, Univ. Twente; 2016. URL: ris.utwente.nl/.../thesis_E_de_Groote.pdf — **Claim:** статический анализ SDF позволяет оценивать производительность и ограниченность; **Evidence:** главы о single‑rate аппроксимациях; **Critique:** ориентирован на SDF, не общий DSL. ([Research Information System][13])
9. Ghamarian AH et al. *Liveness and Boundedness of Synchronous Data Flow Graphs.* FMCAD 2006. URL: sstuijk.estue.nl/.../fmcad06.pdf — **Claim:** алгоритм для проверки liveness и self‑timed boundedness в SDFG; **Evidence:** формальные результаты и алгоритмы; **Critique:** применим к SDF‑подграфам, не к произвольным MoC. ([Sander Stuijk][14])
10. Benveniste A, Berry G. *The Synchronous Languages 12 years later.* (как в [3]) — **Claim:** обсуждает constructive causality как ключевой вклад Esterel; **Evidence:** соответствующие разделы; **Critique:** теория > инструмент. ([Verimag][15])
11. Shiple TM et al. *Constructive Analysis of Cyclic Circuits.* In: *CHARME 1996*. Цитируется в обзорах по constructive circuits. URL: упоминается в lecture09-handout4.pdf — **Claim:** класс constructive circuits определён через трёхзначную семантику и LFP‑итерацию; **Evidence:** логическое/семантическое определение; **Critique:** сфокусировано на логических схемах. ([University of Kiel - Computer Science][4])
12. Colaco JL et al. *A Constructive State-based Semantics and Interpreter for a Data-flow Synchronous Language.* EMSOFT 2023 slides. URL: di.ens.fr/~pouzet/.../emsoft2023-slides.pdf — **Claim:** конструктивная семантика может быть выражена в сильно нормализуемом функциональном языке (Coq), что обеспечивает causality; **Evidence:** изложение семантики и связи с Coq; **Critique:** экспериментальный интерпретатор. ([di.ens.fr][16])
13. Fradet P et al. *RDF: Reconfigurable Dataflow.* DATE 2019. URL: pop-art.inrialpes.fr/~fradet/PDFs/DATE19.pdf — **Claim:** SDF предоставляет статические проверки boundedness/liveness; **Evidence:** обзор SDF как базовой точки; **Critique:** сосредоточен на reconfigurable dataflow. ([pop-art.inrialpes.fr][17])
14. Cremona F et al. *Hybrid co-simulation: it’s about time.* *Software & Systems Modeling* 2019. URL: link.springer.com/article/10.1007/s10270-017-0633-6 — **Claim:** superdense модель времени с целыми индексами решает многие проблемы в гибридной ко‑симуляции; **Evidence:** анализ альтернатив представления времени; **Critique:** сильный фокус на ко‑симуляции, не на DSL. ([SpringerLink][18])
15. Lee EA. *Concurrent Models of Computation for Embedded Software.* 2001. URL: pdfs.semanticscholar.org/... — **Claim:** обсуждает детерминизм/недетерминизм в KPN/SDF/SR/DE‑моделях; **Evidence:** классификация MoC и их свойств; **Critique:** обзорная статья. ([Semantic Scholar][8])

--------------

Это очень ценное дополнение. Ты выделил именно те аспекты, которые отличают "работающий на тестах" фреймворк от **формально корректного**.

Давай разберу каждый пробел и подумаем, как это реализовать практически.

---

## 1. Constructive Causality

Ключевое отличие от "просто fixpoint exists":

**Проблема:** Программа может быть логически непротиворечивой, но требовать "угадать" значение сигнала, а потом проверить, что догадка не привела к противоречию.

**Пример из Esterel:**
```esterel
signal S in
  present S then emit S end
end
```

Логически: `S = S` — два решения: S присутствует или нет. Обе консистентны. Но нет **конструктивного** способа выбрать.

**Как проверять:**

Трёхзначная семантика: `⊥ | 0 | 1` (неизвестно | точно нет | точно да)

```python
def constructive_check(scc: SCC) -> bool:
    """Return True if SCC is constructively causal."""
    
    # Initialize all signals to ⊥
    state = {var: BOTTOM for var in scc.variables}
    
    # Iterate until fixpoint
    for _ in range(MAX_ITERATIONS):
        prev_state = state.copy()
        
        for reaction in scc.reactions:
            # Evaluate reaction in 3-valued logic
            result = reaction.evaluate_3val(state)
            
            # Join results (⊥ ⊔ x = x, 0 ⊔ 1 = error)
            for var, value in result.items():
                state[var] = join_3val(state[var], value)
        
        if state == prev_state:
            break
    
    # Check: any variable still ⊥?
    for var in scc.variables:
        if state[var] == BOTTOM:
            raise NonConstructiveError(
                f"Variable '{var}' cannot be determined constructively"
            )
    
    return True
```

**Практическая сложность:** нужно уметь "интерпретировать" реакции в трёхзначной логике. Для произвольного Python-кода это невозможно. Варианты:

1. **Ограничить выразительность** — реакции как чистые функции с ограниченными операциями
2. **Требовать аннотации** — пользователь декларирует "эта реакция constructive"
3. **Runtime fallback** — статический анализ + runtime detection

---

## 2. Non-Zeno Detection

**Формальный критерий:**
> Для каждого `t`, множество `{(t, μ) | μ ∈ ℕ}` должно быть конечным в любом execution.

**Статический анализ (консервативный):**

```python
def detect_potential_zeno(scc: SCC) -> list[Warning]:
    """Flag SCCs that might cause infinite microsteps."""
    
    warnings = []
    
    for reaction in scc.reactions:
        # Check: does reaction always emit same-time event?
        if reaction.always_emits_same_time():
            # Check: is there a decreasing ranking function?
            if not has_decreasing_rank(reaction, scc):
                warnings.append(ZenoWarning(
                    reaction=reaction,
                    message="Reaction may cause infinite microsteps",
                    fix_hint="Add delay or termination condition"
                ))
    
    return warnings
```

**Ranking function:** Переменная или выражение, которое строго уменьшается на каждой итерации. Например:
- Counter, который декрементируется
- Buffer, который пустеет
- State в конечном автомате, который прогрессирует

Если такой функции нет — warning. Не error, потому что статически неразрешимо полностью.

---

## 3. Детерминированное слияние записей

**Общий принцип:**

```python
class WritePolicy(Protocol):
    def merge(self, values: list[T]) -> T:
        """Deterministically combine multiple writes."""
        ...

# Встроенные политики:
class LastWriterWins(WritePolicy):
    def merge(self, values): return values[-1]  # Needs ordering

class Sum(WritePolicy):
    def merge(self, values): return sum(values)

class Max(WritePolicy):
    def merge(self, values): return max(values)

class Union(WritePolicy):
    def merge(self, values): return set().union(*values)

class Error(WritePolicy):
    def merge(self, values):
        if len(values) > 1:
            raise MultipleWritersError()
        return values[0]
```

**При компиляции:**

```python
def check_write_conflicts(variable: Variable, writers: list[Reaction]):
    if len(writers) <= 1:
        return  # OK
    
    if variable.write_policy is None:
        raise CompileError(
            f"Variable '{variable}' has {len(writers)} writers "
            f"but no merge policy defined",
            fix_hint="Add write_policy=Sum() or write_policy=LWW(order=...)"
        )
    
    if isinstance(variable.write_policy, LastWriterWins):
        if not variable.write_policy.has_ordering():
            raise CompileError(
                f"LWW policy requires explicit ordering of writers",
                fix_hint="Specify priority for each writer"
            )
```

---

## 4. Инициализация vs Bottom vs Absent

**Три разных состояния:**

| Состояние | Значение | Когда |
|-----------|----------|-------|
| `⊥` (bottom) | Не вычислено | Causality failure |
| `absent` | Нет значения в этом такте | Clock не активен |
| `present(v)` | Есть значение `v` | Нормальное состояние |

**Правила:**

1. **State** — всегда `present`. Имеет начальное значение.
2. **Input** — может быть `absent`, если источник на другом clock domain.
3. **Intermediate** — `⊥` во время вычисления, `present` после.

**Проверка:**

```python
def check_initialization(graph: Graph):
    """Ensure no reaction reads ⊥."""
    
    for reaction in topological_order(graph):
        for input_var in reaction.reads:
            
            # Check: is input guaranteed to be present?
            if not is_initialized_before(input_var, reaction):
                raise InitializationError(
                    f"Reaction '{reaction}' may read uninitialized '{input_var}'",
                    fix_hint="Add default value or ensure write happens first"
                )

def is_initialized_before(var: Variable, reaction: Reaction) -> bool:
    """Check if var is guaranteed to have a value when reaction runs."""
    
    # Case 1: State with initial value
    if var.has_initial_value():
        return True
    
    # Case 2: Written by reaction that runs before
    for writer in var.writers:
        if happens_before(writer, reaction):
            return True
    
    # Case 3: Input with default
    if var.has_default():
        return True
    
    return False
```

---

## 5. SDF Boundedness/Liveness

**Когда применимо:** Подграф, где известны production/consumption rates.

```python
@dataclass
class SDFActor:
    inputs: dict[str, int]   # port -> tokens consumed
    outputs: dict[str, int]  # port -> tokens produced

class Downsampler(Node):
    """SDF actor: consumes 10, produces 1"""
    
    input = Input[float](rate=10)   # Consumes 10 per firing
    output = Output[float](rate=1)  # Produces 1 per firing
```

**Анализ:**

```python
def analyze_sdf_subgraph(actors: list[SDFActor], channels: list[Channel]):
    """Check SDF consistency, boundedness, liveness."""
    
    # 1. Build topology matrix Γ
    # Rows = channels, Cols = actors
    # Γ[c,a] = production_rate - consumption_rate
    
    # 2. Solve Γ · q = 0 for positive integer vector q
    # q[a] = number of firings per period
    q = solve_balance_equations(gamma)
    
    if q is None:
        raise SDFError("No consistent firing schedule exists (deadlock)")
    
    # 3. Check boundedness: compute max buffer sizes
    max_buffer = compute_max_buffer_sizes(actors, channels, q)
    
    if any(size == INFINITY for size in max_buffer.values()):
        raise SDFError("Unbounded buffer detected")
    
    # 4. Check liveness: ensure no actor starves
    if not is_live(actors, channels, q):
        raise SDFError("Potential starvation detected")
    
    return SDFSchedule(firing_vector=q, buffer_sizes=max_buffer)
```

---

## 6. Гибридная корректность

**Для ODE-подсистем:**

```python
class ContinuousNode(Node):
    """Node with ODE dynamics."""
    
    @continuous(
        integrator="RK45",
        max_step=0.001,
        lipschitz_bound=10.0  # Декларируем Lipschitz-константу
    )
    def dynamics(self, x, u):
        return -self.a * x + u
```

**Проверки:**

```python
def check_hybrid_correctness(node: ContinuousNode):
    # 1. Lipschitz bound declared?
    if node.lipschitz_bound is None:
        warn("No Lipschitz bound declared; ODE solver may diverge")
    
    # 2. Guards don't cause Zeno?
    for guard in node.guards:
        if guard.can_trigger_immediately_after_reset():
            raise ZenoError(
                f"Guard '{guard}' may trigger immediately after reset",
                fix_hint="Add minimum dwell time or guard delay"
            )
```

---

## 7. Fairness как часть семантики

```python
class SchedulerContract:
    """Formal contract for scheduler behavior."""
    
    # Fairness: any enabled reaction eventually fires
    fairness: Literal["strong", "weak", "none"] = "weak"
    
    # Priority: how to order simultaneous reactions
    priority: Literal["static", "dynamic", "round_robin"] = "static"
    
    # Determinism: same inputs → same execution order
    deterministic: bool = True
```

**При компиляции — проверка, что scheduler contract выполним:**

```python
def check_scheduler_feasibility(reactions: list[Reaction], contract: SchedulerContract):
    if contract.fairness == "strong":
        # Strong fairness: if infinitely often enabled, infinitely often fires
        # Requires that no reaction can block others indefinitely
        
        for r in reactions:
            blockers = reactions_that_can_block(r)
            if blockers and not can_make_progress_around(blockers):
                raise FairnessError(
                    f"Reaction '{r}' may be starved by {blockers}"
                )
```

---

## Обновлённый Compile Pipeline

```
Compile(nodes, connections):

  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 0: Clock Analysis                                     │
  │   - Infer clock domains for each variable/reaction          │
  │   - Check clock compatibility on connections                 │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 1: Structural Correctness                             │
  │   - Type compatibility                                       │
  │   - All inputs connected                                     │
  │   - No dangling references                                   │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 2: Dependency Graph + SCC                             │
  │   - Build edges (instant vs delayed)                         │
  │   - Tarjan SCC decomposition                                 │
  │   - Classify: DAG / algebraic-cycle / delayed-cycle          │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 3: Algebraic SCC Analysis                             │
  │   a) Monotonicity + height bounds                            │
  │   b) CONSTRUCTIVE CAUSALITY (3-valued iteration)             │
  │   c) Write conflict resolution (monoïd / order)              │
  │   d) Zeno risk detection (ranking functions)                 │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 4: Initialization Check                               │
  │   - Every variable has value before first read               │
  │   - Distinguish ⊥ vs absent vs present                       │
  │   - Check default values / guaranteed writes                 │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 5: Dataflow Subgraph Analysis (if applicable)         │
  │   - Detect SDF subgraphs                                     │
  │   - Balance equations                                        │
  │   - Boundedness + liveness                                   │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 6: Hybrid Correctness (if continuous nodes)           │
  │   - ODE existence/uniqueness (Lipschitz)                     │
  │   - Guard Zeno detection                                     │
  │   - Dwell time constraints                                   │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 7: Scheduler Synthesis                                │
  │   - Topological sort of SCCs                                 │
  │   - Static schedule for SDF parts                            │
  │   - Fairness verification                                    │
  │   - Priority assignment                                      │
  └─────────────────────────────────────────────────────────────┘
                              ↓
  ┌─────────────────────────────────────────────────────────────┐
  │ PHASE 8: Code Generation                                    │
  │   - Allocate state                                           │
  │   - Generate execution loop                                  │
  │   - Insert runtime checks (where static analysis incomplete) │
  └─────────────────────────────────────────────────────────────┘
```

-----------

**Restated Question / Переформулировка запроса**

* **EN.** You’ve restated the missing formal aspects (constructive causality, non‑Zeno, deterministic merges, init vs bottom vs absent, SDF boundedness, hybrid correctness, fairness) and sketched how to implement each. Now you’re asking: *what level of formality should this framework actually target in practice—fully academic like Esterel/Lustre, pragmatic but sound, or “best‑effort” like a smart linter? And how do these choices map onto a realistic implementation plan?*
* **RU.** Ты выделил недостающие формальные аспекты (конструктивная каузальность, non‑Zeno, детерминированное слияние, init vs bottom vs absent, SDF‑boundedness, гибридная корректность, fairness) и набросал, как их реализовывать. Теперь вопрос: *какой уровень формальности реально целесообразен — «академический» (как Esterel/Lustre), «прагматично‑sound» или «best effort» — и как это приземлить на реализацию?*

---

## 1. Какой уровень формальности брать — краткий ответ

Я бы предложил такой таргет:

> **Цель: “Pragmatic sound core + escape hatches”.**
> То есть: **ядро DSL** с достаточно строгими ограничениями, где ты реально делаешь конструктивную каузальность, non‑Zeno и init‑анализ, и **”зона Python” вокруг**, где разрешены произвольные эффекты с лучшими доступными проверками и понятными runtime‑ошибками.

Грубо:

* Для **ядра** (чистые реакций, ограниченные операции, формализуемые modes переменных) — максимально близко к *Academic correct* (по мотивам Esterel/Lustre/SR).
* Для всего, что **упирается в произвольный Python / ML‑код / сторонние типы** — уровень **Pragmatic sound** с явно помеченными “чёрными ящиками” и защитой через контракты + runtime guard’ы.
* Уровень **Best effort** оставить только в тех местах, где ты принципиально не можешь анализировать — но сделать так, чтобы пользователь это ясно видел (типа “unsafe zone”).

То есть не выбирать 1/2/3, а **стратифицировать фреймворк** на три концентрических круга:

1. **Safe Core DSL** (строгий, близко к #1).
2. **Extended DSL / Pragmatic** (аннотируемые чёрные ящики, runtime проверки, #2).
3. **Raw Python Escape Hatch** (#3, но ярко помеченный и локализованный).

---

## 2. Разбор твоих 7 дырок — что куда отнести

### 1) Constructive Causality

Твой эскиз с трёхзначной логикой — это прямо то, что делают в constructive‑анализе циклических схем и Esterel:

* На **Safe Core DSL**:

  * Реакции выражаются не произвольным Python, а **ограниченным языком выражений** (AST: `if`, `and/or/not`, простые арифметики, без динамического рефлексивного ада).
  * Для такого AST реально написать `evaluate_3val` и `join_3val`, а затем прогнать фикс‑пойнт **на уровне формул**.
  * Любой SCC, где есть только такие реакции → полный constructive‑check; если остаётся `⊥` — compile‑time ошибка.

* Для **расширенного слоя / Python‑реакций**:

  * Рассматривать их как чёрные ящики:

    * либо пользователь явно объявляет их **монотонными** и без internal‑branching по `⊥` (по сути контракт),
    * либо **запрещаем** их участвовать в zero‑delay циклах (требуем явный delay).
  * То есть статический анализ может сказать: “в этом цикле есть `@blackbox` → либо добавь задержку, либо опиши lattice‑mode и monotone‑contract”.

Это как «типобезопасный» vs «unsafe» блок в языках системного программирования: в **core** всё доказуемо, за его пределами — строго ограниченные паттерны.

---

### 2) Non‑Zeno detection

То, что ты описал (ranking function + “always emits same‑time event”) — это очень хороший **практический** подход.

* Для **core**:

  * Разрешать instant‑self‑trigger только если есть **явно указанная ранговая переменная** (counter, finite‑state) и ты можешь показать, что она **строго убывает** или хотя бы не может образовать бесконечную строгую цепочку на одном `t`.
  * В DSL это можно оформить макросом:

    ```python
    @nonzeno(rank="remaining_steps")
    def reaction(...):
        ...
    ```

* Для **чёрных ящиков**:

  * Консервативно: если SCC состоит только из instant‑edges и чёрных ящиков, без явной ранговой переменной → compile‑time **warning**, runtime watchdog (лимит микрошагов на `t` + понятная ошибка).
  * Это “pragmatic sound”: мы предупреждаем и ставим страховку, но не блокируем все сложные случаи.

---

### 3) Детерминированное слияние записей

Это как раз тот аспект, который **легко** сделать формально строгим везде, даже с Python:

* Требуем, чтобы **каждая переменная** имела явный `write_policy`, который является либо:

  * моноидальной операцией (`Sum`, `Max`, `Union` и т.п.), либо
  * LWW с явно заданным порядком/приоритетами, либо
  * `Error` (по умолчанию, если ты хочешь запретить множественных писателей).

С этим:

* В **core** ты можешь дополнительно проверять, что `write_policy` монотонен относительно решётки (для фикс‑пойнта).
* В **расширенной части** хотя бы гарантируешь детерминизм результата при множественных писателях.

Это один из немногих пунктов, где **можно и нужно быть максимально строгим** без потери удобства.

---

### 4) Init vs ⊥ vs absent

То, как ты это разложил, — почти идеально:

* В **типовой системе DSL** ввести три категории:

  ```text
  State[T]      — всегда present, с initial value (обязателен)
  Input[T]      — может быть absent в такте (clocked)
  Signal[T]     — временные/вычисленные значения (⊥ во время шага, потом present)
  ```

* Статический анализ:

  * Все `State` → обязателен `init=`.
  * Для `Input` → либо пришли с другого узла (проверка clocks), либо есть `default`.
  * Для каждого чтения `x` в реакции — убедиться, что `x` не может быть только `⊥` (либо init, либо write happens‑before, либо default).

Для **чёрных ящиков**:

* Ты можешь объявить: “эта реакция **никогда** не читает свои входы, если они absent/⊥, или умеет с этим жить” — но это уже контракт.

Здесь вполне реально добиться **формальной корректности** для всех реакций, написанных на DSL‑языке, а для Python‑колбэков ввести runtime‑guard (поднимаем исключение, если кто‑то читает `⊥`/absent без default).

---

### 5) SDF boundedness/liveness

Я бы этот кусок сделал **опциональным “режимом” порта**:

* Если пользователь на порте указывает `rate=...` (SDF‑режим) → запускается SDF‑анализ, как ты описал: баланс‑уравнения, boundedness, liveness.
* Если нет — порт живёт в “generic stream”‑режиме без гарантий ограниченности буферов.

Так ты получаешь:

* **Core‑подмножество**: SDF‑подграфы, где можешь гарантировать отсутствие deadlock’а и ограниченность буферов.
* **Остальная часть**: обычный DE/SR‑мир, где работает event‑driven логика и другие механизмы.

Это тоже хороший пример разнесения “академического” кусочка на поддомен, где это действительно окупается.

---

### 6) Гибридная корректность

Честный ответ:

* **Полный** анализ гибридных автоматов (existence/uniqueness ODE, отсутствие Zeno, инварианты) — это исследовательский ад и вряд ли окупится для твоего основного use case (trading/RL).
* Но полезный минимум:

  * Требовать явный `max_step` и, возможно, декларируемый `lipschitz_bound` (как hints solver’у).
  * Структурно запрещать guard’ы, которые могут срабатывать **моментально после собственного reset’а**, если нет таймера/минимального dwell time.
  * Дальше — runtime: логировать подозрительно частые пересечения guard’ов и считать это потенциальным Zeno.

Я бы этот слой держал на уровне **best‑effort + runtime мониторинг**, а не пытался сделать его формально‑строгим.

---

### 7) Fairness / progress

Здесь я бы не пытался делать тяжёлый формальный анализ:

* Зафиксировать **семантику scheduler’а** (например, статический порядок/приоритет + round‑robin среди равноправных реакций).
* Документировать его как часть модели вычислений.
* Для сложных сценариев (типа actor‑based RL) полагаться на дизайн правил/очередей, а не на формальное доказательство fairness.

То есть это скорее **семантический контракт + дизайн**, чем строгий критерий компилятора.

---

## 3. Практический план: как «нарезать» формальность по слоям

### Шаг 0. Ввести разделение уровней

В терминах API:

```python
# 1) Safe Core DSL: жёстко ограниченный, анализируемый
class CoreNode(Node):
    @reaction  # принимает/возвращает Expr-ы DSL, а не произвольный Python
    def step(self, ...):
        ...

# 2) Extended DSL: чёрные ящики с контрактами
class BoxNode(Node):
    @blackbox(monotone=True, no_instant_loop=True)
    def step(self, snapshot):
        ...

# 3) Raw Python escape
class RawNode(Node):
    @unsafe  # компилятор практически ничего не гарантирует
    def step(self, snapshot):
        ...
```

Дальше:

* **Для CoreNode** — максимальный набор статических проверок (constructive, non‑Zeno, init, merges, SDF).
* **Для BoxNode** — проверяем только то, что совместимо с контрактами (например, можно включать в SCC, если помечен monotone+inflationary).
* **Для RawNode** — почти только структурные проверки + runtime guards (лимит microstep, детектор чтения ⊥, logging).

---

### Шаг 1. Что реализовать в первой версии (минимальный «формальный» фреймворк)

Я бы для MVP сделал:

1. **Структурный + init‑анализ** (включая bottom vs absent, merge‑policy).
2. **SCC‑анализ**:

   * Если цикл zero‑delay и состоит только из Core‑реакций и monotone‑мodes → запуск трёхзначной fixed‑point итерации → либо ок, либо *NonConstructiveError*.
   * Если в цикле есть Box/Raw → **запрещаем** zero‑delay; требуем delay хотя бы в одном ребре.
3. **Non‑Zeno защита**: лимит microstep’ов + структурные предупреждения по явным self‑trigger без rank.
4. **SDF‑анализ** только если пользователь явно включает `rate=` на портах.

Это уже даст очень сильную гарантию для подмножества пользователей и заметно уменьшит класс скрытых багов.

---

### Шаг 2. Эволюция к более “академичному” ядру

По мере того как DSL стабилизируется:

* Ужесточать Core‑слой (например, запретить некоторые конструкции в реакциях, чтобы облегчить анализ).
* Постепенно переносить всё больше доменной логики из Raw/Box в Core, где она попадает под конструктивный и non‑Zeno анализы.
* Документировать это как **две “скорости” разработки**:

  * Быстрый прототип → Raw/Box.
  * Production‑критика → переписываем логику в Core‑стиле и получаем формальные гарантии.

---

## 4. Ответ на твой вопрос прямым текстом

> **Какой уровень формальности тебе нужен?**

С учётом твоих доменов (трейдинг, RL, исследовательская работа, high‑end инженерия, но не safety‑critical авионика) я бы поставил цель:

* **Ядро**: максимально близко к **Academic correct**, но **только для ограниченного подъязыка**, где это реально — и где компилятор может дать жёсткие гарантии.
* **Вокруг ядра**: **Pragmatic sound** — консервативный анализ, чёткие контракты, runtime страховки.
* **Любой “чистый Python”** — осознанно в зоне **Best effort**, явно помеченной в API.

В терминах твоих опций:

* Не “выбрать один из трёх”, а **скомбинировать**:

  * (1) *внутри core DSL*,
  * (2) *как основной стиль для фреймворка в целом*,
  * (3) *как контролируемый escape hatch*.

---

## 5. Короткая расшифровка терминов

* **Constructive causality** — требование, что значения сигналов можно вычислить *конструктивно*, без угадывания, с помощью монотонной итерации в трёхзначной логике.
* **Non‑Zeno** — гарантия, что на одном логическом времени `t` не будет бесконечной цепочки microstep‑ов (бесконечных instant‑реакций).
* **Monoid merge** — политика объединения записей, когда есть ассоциативная, коммутативная операция с нейтральным элементом (`sum`, `max`, `union`).
* **SDF boundedness/liveness** — для Synchronous Dataflow‑подграфов: есть статический schedule, буферы конечны, акторы не застревают.
* **Hybrid correctness** — корректность гибридных (ODE+дискрет) подсистем: существование/единственность решений и отсутствие Zeno.
* **Fairness** — свойство scheduler’а: реакция, чьи условия включения выполняются бесконечно часто, не должна быть навсегда зажата другими.

---------


Это **точно правильный** подход. Стратификация вместо единого уровня — это то, что делают зрелые системы (Rust с `unsafe`, TypeScript с `any`, Scala с macro/reflection boundaries).

Давай конкретизирую, как может выглядеть **Core DSL** с ограниченным языком выражений, который реально поддаётся constructive analysis.

---

## Core DSL: Ограниченный язык для формальных гарантий

Ключевая идея: **реакции в Core — это не произвольный Python, а embedded DSL**, который компилируется в Python, но имеет формальную семантику.

### Вариант 1: Expression-based DSL

```python
from regelum.core import Node, State, Input, Output, Expr
from regelum.core.ops import if_, and_, or_, not_, clamp, sum_

class ThermostatCore(Node):
    """Полностью анализируемая нода."""
    
    temperature = Input[float]()
    heater_on = State[bool](init=False)
    power = Output[float]()
    
    # Параметры — константы, известны на этапе компиляции
    low: float = 18.0
    high: float = 22.0
    
    @reaction
    def control(self, temperature: Expr[float], heater_on: Expr[bool]) -> Expr[float]:
        # Это НЕ Python if — это DSL-конструкция
        new_state = if_(
            temperature < self.low,
            True,                           # включить
            if_(temperature > self.high,
                False,                      # выключить
                heater_on)                  # сохранить
        )
        
        # Явно указываем, что пишем в state
        self.heater_on.set(new_state)
        
        # Return — это output
        return if_(new_state, 1.0, 0.0)
```

**Что это даёт:**
- `Expr[T]` — это AST-нода, не значение
- Компилятор может:
  - Построить dependency graph внутри реакции
  - Запустить 3-valued evaluation
  - Проверить constructive causality
  - Сгенерировать оптимальный код

### Вариант 2: Dataclass + Pattern Matching (более Pythonic)

```python
from regelum.core import Node, State, Input, Output
from regelum.core import Match, Case, when

class ThermostatCore(Node):
    temperature = Input[float]()
    heater_on = State[bool](init=False)
    power = Output[float]()
    
    low: float = 18.0
    high: float = 22.0
    
    @reaction
    def control(self):
        # Pattern matching — ограниченный, анализируемый
        return Match(
            Case(self.temperature < self.low,
                 then=dict(heater_on=True, power=1.0)),
            
            Case(self.temperature > self.high,
                 then=dict(heater_on=False, power=0.0)),
            
            Case(otherwise,
                 then=dict(heater_on=self.heater_on,  # keep
                           power=when(self.heater_on, 1.0, 0.0)))
        )
```

### Вариант 3: Декларативные правила (table-driven)

```python
class ThermostatCore(Node):
    temperature = Input[float]()
    heater_on = State[bool](init=False)
    power = Output[float]()
    
    @rules
    def control(self):
        return [
            # condition        → state updates
            (temp < 18.0,       {"heater_on": True}),
            (temp > 22.0,       {"heater_on": False}),
            (heater_on == True, {"power": 1.0}),
            (heater_on == False,{"power": 0.0}),
        ]
```

Это ещё более декларативно и проще анализировать.

---

## Трёхзначная семантика: Как это реально работает

```python
from enum import Enum
from typing import TypeVar, Generic

T = TypeVar('T')

class Logic3(Enum):
    BOTTOM = "⊥"      # Неизвестно
    FALSE = "0"
    TRUE = "1"

class Value3(Generic[T]):
    """Трёхзначное значение."""
    
    def __init__(self, value: T | None, known: bool):
        self._value = value
        self._known = known
    
    @classmethod
    def bottom(cls) -> "Value3[T]":
        return cls(None, False)
    
    @classmethod
    def known(cls, value: T) -> "Value3[T]":
        return cls(value, True)
    
    def is_bottom(self) -> bool:
        return not self._known


def evaluate_3val(expr: Expr, env: dict[str, Value3]) -> Value3:
    """Evaluate expression in 3-valued logic."""
    
    match expr:
        case Const(value):
            return Value3.known(value)
        
        case Var(name):
            return env.get(name, Value3.bottom())
        
        case BinOp("+", left, right):
            l = evaluate_3val(left, env)
            r = evaluate_3val(right, env)
            if l.is_bottom() or r.is_bottom():
                return Value3.bottom()
            return Value3.known(l.value + r.value)
        
        case If(cond, then_, else_):
            c = evaluate_3val(cond, env)
            
            if c.is_bottom():
                # Оба бранча могут дать информацию!
                t = evaluate_3val(then_, env)
                e = evaluate_3val(else_, env)
                
                # Если оба известны и равны → известно
                if not t.is_bottom() and not e.is_bottom():
                    if t.value == e.value:
                        return t
                
                return Value3.bottom()
            
            if c.value:
                return evaluate_3val(then_, env)
            else:
                return evaluate_3val(else_, env)
        
        case Lt(left, right):  # <
            l = evaluate_3val(left, env)
            r = evaluate_3val(right, env)
            
            if l.is_bottom() or r.is_bottom():
                return Value3.bottom()
            
            return Value3.known(l.value < r.value)


def check_constructive_causality(scc: SCC) -> bool:
    """Verify SCC is constructively causal."""
    
    # Initialize all outputs to ⊥
    env = {var.name: Value3.bottom() for var in scc.written_variables}
    
    # Add known inputs (from outside SCC)
    for var in scc.read_only_variables:
        env[var.name] = Value3.known(var.initial_or_default)
    
    # Fixed-point iteration
    for iteration in range(MAX_ITERATIONS):
        changed = False
        
        for reaction in scc.reactions:
            for output_var, expr in reaction.outputs.items():
                new_value = evaluate_3val(expr, env)
                old_value = env[output_var]
                
                # Join: ⊥ ⊔ x = x, x ⊔ x = x
                if old_value.is_bottom() and not new_value.is_bottom():
                    env[output_var] = new_value
                    changed = True
                elif not old_value.is_bottom() and not new_value.is_bottom():
                    if old_value.value != new_value.value:
                        raise NonDeterministicError(
                            f"Variable {output_var} has conflicting values"
                        )
        
        if not changed:
            break
    
    # Check: any variable still ⊥?
    for var_name, value in env.items():
        if value.is_bottom():
            raise NonConstructiveError(
                f"Variable '{var_name}' cannot be determined constructively.\n"
                f"The reaction depends on a value that cannot be computed "
                f"without 'guessing'.\n"
                f"Fix: Add explicit default or break the cycle with delay."
            )
    
    return True
```

---

## Границы между уровнями: API

```python
# ═══════════════════════════════════════════════════════════════
# LEVEL 1: Core DSL (полные гарантии)
# ═══════════════════════════════════════════════════════════════

from regelum.core import CoreNode, State, Input, Output

class SafeController(CoreNode):
    """Все реакции — анализируемые Expr."""
    
    x = Input[float]()
    y = Output[float]()
    
    @reaction
    def compute(self, x):
        return x * 2.0  # Это Expr, не Python float


# ═══════════════════════════════════════════════════════════════
# LEVEL 2: Extended (контракты + частичные гарантии)
# ═══════════════════════════════════════════════════════════════

from regelum.extended import ExtNode, contract

class MLController(ExtNode):
    """Содержит black-box Python, но с контрактами."""
    
    observation = Input[Tensor]()
    action = Output[Tensor]()
    
    def __init__(self, policy: nn.Module):
        self.policy = policy
    
    @reaction
    @contract(
        monotone=False,           # Не участвует в fixpoint
        deterministic=True,       # Тот же вход → тот же выход
        no_side_effects=True,     # Не меняет внешнее состояние
        max_latency_ms=10,        # Hint для scheduler
    )
    def infer(self, observation: Tensor) -> Tensor:
        # Произвольный Python, но компилятор знает контракт
        with torch.no_grad():
            return self.policy(observation)


# ═══════════════════════════════════════════════════════════════
# LEVEL 3: Raw Escape Hatch (минимальные гарантии)
# ═══════════════════════════════════════════════════════════════

from regelum.raw import RawNode, unsafe

class LegacyIntegration(RawNode):
    """Интеграция со старым кодом."""
    
    input = Input[Any]()
    output = Output[Any]()
    
    @reaction
    @unsafe(reason="Legacy API with unknown semantics")
    def process(self, ctx):
        # Полный доступ к ctx, произвольный Python
        # Компилятор только:
        # - Проверяет структуру (input/output connected)
        # - Ставит runtime guards (microstep limit, timeout)
        result = legacy_system.process(ctx.read(self.input))
        ctx.emit(self.output, result)
```

---

## Compile-time сообщения по уровням

```
═══════════════════════════════════════════════════════════════════
REGELUM COMPILATION REPORT
═══════════════════════════════════════════════════════════════════

Nodes analyzed: 12
  ├── Core DSL:     7  (full formal guarantees)
  ├── Extended:     4  (contract-based guarantees)
  └── Raw:          1  (runtime guards only)

───────────────────────────────────────────────────────────────────
CORE ANALYSIS RESULTS
───────────────────────────────────────────────────────────────────

✓ Structural correctness:     PASS
✓ Type compatibility:         PASS
✓ Initialization analysis:    PASS
✓ Constructive causality:     PASS (2 SCCs verified)
✓ Non-Zeno check:             PASS (ranking functions found)
✓ Write conflict resolution:  PASS (all policies defined)

───────────────────────────────────────────────────────────────────
EXTENDED NODES
───────────────────────────────────────────────────────────────────

⚠ MLController.infer:
    Contract: deterministic=True, no_side_effects=True
    Status:   Cannot verify statically; runtime monitoring enabled
    
⚠ DataProcessor.transform:
    Contract: monotone=True
    Status:   Participating in SCC #3; contract TRUSTED

───────────────────────────────────────────────────────────────────
RAW NODES (reduced guarantees)
───────────────────────────────────────────────────────────────────

⚠ LegacyIntegration.process:
    Reason:   "Legacy API with unknown semantics"
    Guards:   microstep_limit=1000, timeout_ms=100
    Warning:  Cannot guarantee determinism or termination

───────────────────────────────────────────────────────────────────
SCHEDULE SYNTHESIS
───────────────────────────────────────────────────────────────────

Static schedule:  Partially computed (SDF subgraph: 5 actors)
Buffer bounds:    {channel_1: 10, channel_2: 32, channel_3: 1}
Event-driven:     7 reactions (DE semantics)

═══════════════════════════════════════════════════════════════════
COMPILATION: SUCCESS (with 3 warnings)
═══════════════════════════════════════════════════════════════════
```

---

## Резюме

Твой подход к стратификации — это **правильный ответ**. Конкретно:

| Аспект | Core | Extended | Raw |
|--------|------|----------|-----|
| Constructive causality | ✓ 3-valued proof | Trust contract | Runtime guard |
| Non-Zeno | ✓ Ranking verified | Warn + runtime | Runtime limit |
| Write conflicts | ✓ Monoïd verified | ✓ Policy required | ✓ Policy required |
| Init analysis | ✓ Proven | ✓ Proven | Runtime check |
| SDF | ✓ Full analysis | If annotated | N/A |
| Hybrid | Dwell-time check | Lipschitz hint | Runtime monitor |

**Главный принцип:** формальные гарантии *где возможно*, явные контракты + runtime guards *везде остальном*, и чёткая граница между уровнями.


----------

**Restated Question / Переформулировка запроса**

* **EN.** *Given the concrete Core DSL design you sketched (expression‑based `Expr` AST, pattern‑matching and table‑driven rules, three‑valued semantics and fixed‑point analysis), is this the right shape for a constructively analyzable language? What needs to be tightened, changed, or added so that the compiler can really prove constructive causality, non‑Zeno behavior, and determinism, while Extended/Raw layers stay usable?*
* **RU.** *Учитывая набросанный тобой Core DSL (AST `Expr`, варианты синтаксиса — выражения, pattern‑matching, rules‑таблицы, трёхзначная семантика и фикс‑пойнт‑анализ), насколько это правильно с точки зрения формальной конструктивной семантики? Что нужно докрутить/подправить/добавить, чтобы компилятор реально мог доказывать конструктивную каузальность, non‑Zeno и детерминизм, а расширенные/сырые слои оставались удобными?*

---

## 1. В целом: ты попал прямо в “канонический” дизайн

**Тезис 1.** *“Reactions as an embedded expression language (AST, не произвольный Python)”* — это ровно та форма, через которую Esterel/Scade/Polychrony приходят к конструктивной семантике: ограниченный синтаксис → трёхзначная логика → наименьшая неподвижная точка как конструктивное значение.¹ ² ³ ([Inria Côte d'Azur][1])

**Тезис 2.** *Pattern‑matching и таблицы правил* логически сводимы к тому же AST (набор вложенных `if`/`case`), и это супер‑удобный “surface”‑синтаксис при сохранении строгой внутренней семантики.² ([Ptolemy Project][2])

Так что направление «Core DSL = выражения/паттерны/таблицы, собирающиеся в AST по чёткой грамматике» — правильное. Главный вопрос теперь не “да/нет”, а **как зафиксировать минимум ограничений**, чтобы:

* анализ по трёхзначной логике **точно был монотонен** и сходился;
* совпадал с тем, что литература называет **constructive causality**;¹ ² ³ ([Inria Côte d'Azur][1])
* при этом оставался достаточно выразительным.

---

## 2. Core DSL как AST: что стоит “застеклить”

### 2.1. Типизированный AST `Expr[T]` с phantom‑типами

**Тезис 3.** Чтобы статический анализ не превратился в ад‑if‑else, разумно сделать `Expr[T]` *типизированным AST* с phantom‑типами, как в Coq/Agda‑подобных DSL (или как в некоторых FRP/DSL‑ах).¹ ² ([Ptolemy Project][2])

Минимальный набор узлов:

```python
class Expr(Generic[T]): ...
class Const(Expr[T]): value: T
class Var(Expr[T]):   name: str
class If(Expr[T]):    cond: Expr[bool]; then_: Expr[T]; else_: Expr[T]

class BinOp(Expr[T]): op: Literal["+", "-", "*", ...]; left: Expr[T]; right: Expr[T]
class Cmp(Expr[bool]): op: Literal["<", "<=", "==", ...]; left: Expr[Any]; right: Expr[Any]
# Специальные узлы для present/absent, when, merge и т.д.
```

Твои `if_`, `and_`, `or_`, `clamp`, `sum_` и т.п. в таком дизайне — просто **конструкторы AST**, а не реальный код.

### 2.2. Pattern‑matching и таблицы — сахар

**Тезис 4.** `Match/Case/when` и таблицы правил (`[(cond, then_dict), ...]`) можно рассматривать как **синтаксический сахар**:

* `Match(Case(...), Case(...), ...)` → вложенный `If`
* Таблица правил → композиция `If..elif..else` над обновлением полей `State/Output`.

Это хорошо, потому что **анализу всё равно** — он ходит по уже нормализованному AST (с `If`, `Var`, `Const`, `BinOp`, …).² ([Ptolemy Project][2])

---

## 3. Трёхзначная семантика: где докрутить твой эскиз

Ты очень близко повторил схемы из работ Berry/Schneider/школы Esterel: трёхзначная логика (`⊥/false/true`), частичный порядок `⊥ ⊑ 0,1`, итерация до неподвижной точки.¹ ³ ⁴ ([Inria Côte d'Azur][1])

Есть несколько моментов, которые важно зафиксировать:

### 3.1. Разделить:

* **Boolean‑уровень** (present/absent, guards) — классическая Kleene‑логика.¹ ³ ⁴ ([Inria Côte d'Azur][1])
* **Value‑уровень** (числа, структуры) — здесь трёхзначность нужна только как “неизвестно/известно v”, без 0/1.

Можно сделать два слоя:

```python
class B3(Enum):
    BOTTOM = "⊥"
    FALSE = "0"
    TRUE = "1"

@dataclass
class V3(Generic[T]):
    # либо известное значение (present v), либо ⊥
    value: T | None
    known: bool
```

Тогда:

* Все **условия/guards** работают на `B3` с Kleene‑семантикой.¹ ³ ([Inria Côte d'Azur][1])
* Все **данные** — через `V3[T]` (bottom vs known(v)).

Твой `Value3` — по сути `V3`; просто полезно явно отделить boolean‑слой.

### 3.2. Semantics `If` при `cond = ⊥`

То, что ты написал:

> Если `cond = ⊥`, запросить оба бранча, если оба известны и равны → взять это значение, иначе `⊥`.

— это нормальная **конструктивная** семантика if‑а; именно так в лекциях по constructive causality обычно мотивируют необходимость 3‑значной логики.³ ⁴ ([Kiel University - Computer Science][3])

Важно, чтобы:

* `evaluate_3val` был **монотонным** по отношению к `⊥ ⊑ v` (т.е. чем больше информации в env, тем больше/не меньше в результате).¹ ⁴ ([Inria Côte d'Azur][1])
* Ты **не вводил операций, которые ломают монотонность** (например, читать “отрицание неизвестности” `is_unknown(x)` без guard’ов).

### 3.3. Merge и multi‑write

В `check_constructive_causality`:

```python
if old_value.is_bottom() and not new_value.is_bottom(): ...
elif not old_value.is_bottom() and not new_value.is_bottom():
    if old_value.value != new_value.value:
        raise NonDeterministicError(...)
```

Это хорошо, но:

* Здесь ты фактически зашил политику `Error` для множественных писателей.
* Если хочешь поддерживать `Sum/Max/Union/LWW`, нужно сюда подмешать **политику merge**, а не просто “один или ошибка”.

То есть для каждой переменной в SCC:

```python
merged = policy.join(old_value, new_value)  # моноид / LWW / error
```

А ещё лучше — при построении SCC ты **знаешь всех писателей переменной** и можешь сразу отловить случаи, где политика отсутствует.

---

## 4. Алгоритм конструктивного анализа SCC — что добавить

Твой алгоритм:

* env: все внутренние переменные → `⊥`
* внешние входы → `known(initial_or_default)`
* итерация: `evaluate_3val` для каждого expr → join
* если что‑то осталось `⊥` → `NonConstructiveError`.

Это почти textbook‑вариант “ternary simulation for causality”.³ ⁴ ([Kiel University - Computer Science][3])

Я бы добавил:

1. **Проверку монотонности eval’а** — не в общем виде (сложно), а через ограничение допустимых примитивов:

   * арифметика над `V3` (если любой аргумент ⊥ — результат ⊥);
   * логические операции над `B3` с Kleene‑таблицами;
   * guards вида `when(cond, expr)` → если cond=FALSE → считаем, что expr не влияет (можно вернуть ⊥₂, не повышая информации).

   Всё это можно **закодировать в наборе AST‑узлов**, и тогда монотонность следует из конструкции.¹ ³ ([Inria Côte d'Azur][1])

2. **Отдельную диагностику для `⊥`**:
   Одно дело — `⊥` потому что “нет информации о входе”, другое — потому что “есть логический цикл без решения”. Для UX полезно:

   * показывать цепочку зависимостей, которая приводит к `⊥`;
   * явно писать: “значение `x` зависит от самого себя через instant‑цикл без delay”.

---

## 5. Временная семантика: где в DSL появляются `t` и `t-1`

Пока всё, что ты описал, — **семантика одного тика** (instant) SR/DE‑домена. Реально во фреймворке:

* `State` = хранит `x[t]`, а в реакции ты вычисляешь `x[t+1]` (или `x[t]` в SR‑стиле);
* `Delay`‑узлы явно выражают “здесь есть 1 шаг задержки” (перевод из тип B циклов).

Я бы явно ввёл в Core DSL **примитив задержки**:

```python
class Delay(Expr[T]):
    inner: Expr[T]
    # "take previous value of inner" или ссылку на state
```

И сказал:

> **Тезис 5.** Любой цикл, который проходит только через `State/Delay`, считается **временным** (delay feedback) и не требует алгебраического фикс‑пойнта; конструктивный анализ нужен только для циклов, состоящих из чистых instant‑выражений.² ([Ptolemy Project][2])

Это соответствует тому, как разделяют **algebraic loops vs delayed feedback** в Simulink и SR‑языках.² ([Ptolemy Project][2])

---

## 6. Core / Extended / Raw: как твой дизайн ложится на стратификацию

Я бы суммировал так:

### Core (`CoreNode`)

* Реакции принимают/возвращают только `Expr[T]`.
* Полный набор проверок:

  * структурная + типовая корректность,
  * init vs ⊥ vs absent,
  * SCC + constructive causality (3‑значная итерация),
  * merge‑policy,
  * базовая non‑Zeno‑проверка для instant‑циклов.

Это “Esterel‑подобный” мир с формальными гарантиями.¹ ² ³ ([Inria Côte d'Azur][1])

### Extended (`ExtNode` + `@contract`)

* Реакции пишутся произвольным Python, но **ограничены контрактами**: `monotone`, `no_side_effects`, `max_latency`, `no_instant_loop`.
* В SCC можно включать только те black‑box узлы, которые помечены как `monotone + inflationary` (или наоборот — вообще запретить их в zero‑delay циклах).³ ⁴ ([Kiel University - Computer Science][3])
* Консервативно: если в SCC есть black‑box без подходящего контракта → требуется delay.

### Raw (`RawNode` + `@unsafe`)

* Структурные проверки и runtime‑guards (лимит microsteps, таймауты, лог).
* Отдельно в отчёте компиляции помечается: “здесь формальных гарантий нет”.

---

## 7. Что бы я сделал следующим шагом (именно в коде)

1. **Зашить минимальный, но строгий AST:**

   * `Const, Var, If, BinOp, Cmp, When, Merge, Delay`.
   * Типизированный `Expr[T]` + проверка типов при построении AST.

2. **Сделать простейший CoreNode:**

   * `@reaction` превращает Python‑функцию в построитель AST (через combinator‑API, как ты показал).
   * `self.state.set(expr)` записывает **узел присваивания** в IR, а не вызывает Python‑код.

3. **Реализовать 3‑значный интерпретатор для этого AST:**

   * Сначала без SDF/гибридности, только для instant‑частей.
   * Написать несколько искусственных SCC‑примеров (в т.ч. неконструктивных) и убедиться, что анализ даёт “правильный” флаг.

4. **Встроить merge‑policy:**

   * По умолчанию `Error`;
   * Добавить `Sum`, `Max`, `Union`, `LWW(order=...)`;
   * В анализе SCC учитывать их.

5. **Сделать Extended/Raw только оболочками:**

   * Extended: просто не участвуют в SCC‑аналитике как instant‑части (требуется delay) или участвуют по контракту;
   * Raw: только runtime.

---

## Термины (уточнения)

* **AST (Abstract Syntax Tree)** — структурное представление выражения, где каждая операция/конструкция — отдельный узел дерева; по нему удобно делать статический анализ и генерацию кода.
* **Expr[T]** — типизированный AST‑узел для значения типа `T` (float/bool/…); phantom‑тип гарантирует, что операции только над совместимыми типами.
* **Трёхзначная логика (Kleene)** — логика с тремя значениями `⊥, 0, 1` и частичным порядком по “информативности” (`⊥ ⊑ 0,1`); используется для конструктивного анализа циклов.¹ ³ ⁴ ([Inria Côte d'Azur][1])
* **Constructive causality** — требование, что значения сигналов можно вычислить монотонной итерацией от `⊥` без угадывания; программы, где нужно “предположить и проверить”, отвергаются.¹ ³ ([Inria Côte d'Azur][1])
* **Instant / algebraic cycle** — цикл без задержки (feedback в том же тике); требует фикс‑пойнт анализа.
* **Delay feedback** — цикл, в котором есть хотя бы одна задержка; вычисляется по времени без алгебраических проблем.² ([Ptolemy Project][2])

---

## Ссылки (Vancouver) + краткая связь с твоим дизайном

1. Berry G. *The Constructive Semantics of Pure Esterel.* Draft v3; 2002. URL: [https://www-sop.inria.fr/members/Gerard.Berry/Papers/EsterelConstructiveBook.pdf](https://www-sop.inria.fr/members/Gerard.Berry/Papers/EsterelConstructiveBook.pdf) — **Claim:** даёт конструктивную (трёхзначную) семантику Esterel и алгоритм причинностного анализа; **Evidence:** определение трёхзначной логики и fixed‑point итерации; **Critique:** ориентирован на чисто сигналовую подмножину. ([Inria Côte d'Azur][1])
2. Edwards SA. *Synchronous-Reactive Models.* In: Ptolemaeus C, ed. *System Design, Modeling, and Simulation using Ptolemy II.* 2014. URL: [https://ptolemy.berkeley.edu/books/Systems/chapters/SynchronousReactiveModels.pdf](https://ptolemy.berkeley.edu/books/Systems/chapters/SynchronousReactiveModels.pdf) — **Claim:** описывает SR‑модель (reactions per logical tick, instant vs delayed зависимости, algebraic loops); **Evidence:** глава о синхронных моделях; **Critique:** без деталей языка выражений. ([Ptolemy Project][2])
3. Benveniste A, Berry G, Caspi P, et al. *The synchronous languages 12 years later.* *Proc IEEE.* 2003;91(1):64–83. URL: [https://www-verimag.imag.fr/~halbwach/PS/iee03.pdf](https://www-verimag.imag.fr/~halbwach/PS/iee03.pdf) — **Claim:** формализует понятие constructive causality в Esterel и связь с трёхзначной семантикой; **Evidence:** разделы о causality и constructive semantics; **Critique:** обзор, не конкретный DSL. ([Verimag][4])
4. Schneider K. *Improving Constructiveness in Code Generators.* SBST 2005. URL: [https://privat.thomas-tuerk.de/assets/publications/SBST05a.pdf](https://privat.thomas-tuerk.de/assets/publications/SBST05a.pdf) — **Claim:** описывает causality‑анализ на трёхзначной логике и “ternary simulation” для циклов; **Evidence:** раздел “Causality Analysis Based on Three-Valued Logic”; **Critique:** фокус на генераторах кода. ([privat.thomas-tuerk.de][5])
5. Lecture notes. *Synchronous Languages—Lecture 09 (Constructive Value Propagation).* Univ. Kiel; 2015. URL: [https://www.informatik.uni-kiel.de/~rvh/ss15/v-synch/lectures/lecture09-handout4.pdf](https://www.informatik.uni-kiel.de/~rvh/ss15/v-synch/lectures/lecture09-handout4.pdf) — **Claim:** излагает трёхзначную семантику и фикс‑пойнт‑итерацию как основу конструктивной каузальности; **Evidence:** слайды и формальные определения; **Critique:** учебный формат, не production‑код. ([Kiel University - Computer Science][3])

--------

Насколько меня это будет ограничивать как разработчика? Смогу ли я например не пользоваться всем этим синтаксисом, если например у меня, например, тривиальный случай, вырождающийся в DAG и я сам об этом заранее знаю? 


-------

**Restated Question / Переформулировка запроса**

* **EN.** *How restrictive is this Core DSL in practice? If I know that my particular pipeline is a trivial DAG (no instantaneous cycles), can I avoid all this Core‑syntax and just write “normal” code without paying the full constructive‑semantics tax?*
* **RU.** *Насколько Core‑DSL будет ограничивать меня как разработчика? Если я заранее знаю, что мой пайплайн вырождается в простой DAG (без мгновенных циклов), могу ли я не пользоваться всем этим “строгим” синтаксисом и жить как с обычной библиотекой?*

---

## 1. Коротко: ограничения включаются **только там, где реально нужны**

**Тезис.** В разумной архитектуре Core/Extended/Raw:

* Если **граф реально DAG по мгновенным зависимостям** (т.е. нет zero‑delay циклов),
  → **конструктивный анализ не нужен вообще**, достаточно обычной топологической сортировки.¹
  → Внутри нод ты можешь писать **произвольный Python** (хотя бы в Extended/Raw узлах).

**Только** если компилятор находит **сильно связный компонент (SCC) из zero‑delay рёбер**, он говорит:

> «Вот тут есть algebraic feedback.
> Для узлов в этом SCC либо:
>
> * напиши реакцию в Core DSL,
> * либо явно поставь delay/память,
> * либо возьми на себя контракт/unsafe‑заверение.»

Иными словами:

> Для чистого DAG‑кейса ты не обязан трогать Core‑синтаксис совсем; он включается **по demand**, когда появляются сложные циклы.

Это очень похоже на то, как в синхронных языках **чисто комбинационные схемы**/графы живут без тяжёлого анализа, а вся математика нужна только для циклических частей.¹²

---

## 2. Как это выглядит в реальном флоу

### Сценарий 1: Чистый DAG

```text
A  →  B  →  C
```

* Никаких обратных рёбер без задержки.
* Компилятор строит граф, видит, что нет SCC с более чем одной вершиной по instant‑ребрам → просто делает **топологическую сортировку**.¹
* Внутри `A/B/C` ты можешь:

  * использовать обычный Python‑метод `step(self, snapshot)`,
  * использовать любые типы из PyTorch/NumPy/whatever,
  * не использовать `Expr`, `Match`, `rules` и т.п. вообще.

Единственные проверки:

* структурные (типы портов, все инпуты подключены),
* может быть InitCheck, чтобы не было чтения явно неинициализированного состояния.

**Core‑DSL и трёхзначная логика вообще не участвуют.**

---

### Сценарий 2: Появился instant feedback

```text
A → B → C
↑       │
└───────┘   (обратная связь без задержки)
```

Тут уже:

* Граф по мгновенным зависимостям имеет нетривиальный SCC `{A,B,C}`.²
* И вот для **этого SCC** компилятор говорит:

  * либо ты разрываешь цикл delay’ем (`x[t+1]` вместо `x[t]`),
  * либо описываешь реакции `A/B/C` так, чтобы они были анализируемы (Core DSL или сильные контракты),
  * либо честно ставишь `unsafe`/`@blackbox` и живёшь без гарантий + runtime‑guard.

То есть ограничение **локализовано**: оно касается только подграфа, где реально нужна фикс‑пойнт‑семантика.

---

## 3. Насколько это “ломает руки” как разработчику

### Где ты **ограничен** (если хочешь формальные гарантии)

Внутри **CoreNode** / Core‑реакций:

* Нельзя произвольный Python:
  только `Expr`‑операции (`if_`, логика, арифметика, pattern‑matching, `Match/Case` и т.п.)
  → всё должно собираться в AST, а не исполняться сразу.
* Нельзя скрытые сайд‑эффекты:
  реакция — чистая функция `inputs, state[t] → state[t+1], outputs`.
* Нельзя произвольные типы без определения, как с ними работать в DSL:
  для `Tensor` нужен хотя бы “черезмерный” режим: он просто `present/⊥` и сравнивается/комбинируется только там, где это безопасно.

Зато за это ты получаешь:

* доказуемую конструктивность,
* отсутствие алгебраических петель без delay,
* чёткие init/merge‑гарантии.

### Где ты **свободен**

* Для **DAG‑частей** графа: спокойно используешь Extended/Raw узлы и обычный Python:

  * чистое вычисление логики,
  * ML‑инференс,
  * произвольные типы и библиотеки.
* Даже если в графе есть циклы, **узлы вне SCC** всё равно остаются свободными — Core‑ограничения нужны только для тех, кто реально сидит в мгновенной петле.

В этом смысле Core DSL — это **не глобальная дубинка**, а точечный инструмент: «здесь сложный реактивный цикл → давай опишем его в строгой нотации».

---

## 4. Можно ли “просто сказать компилятору: это DAG, поверь мне”?

Да, и это даже лучше сделать не как “верь”, а как **assert + проверка**:

```python
g = Graph(assume_acyclic=True)
...
g.compile()
```

* Если граф правда DAG по instant‑зависимостям → компилятор это подтверждает, падает в простой режим, **никакого Core DSL внутри не требует**.¹
* Если окажется, что есть цикл → кидаем CompileError:
  «заявлен режим `assume_acyclic`, но обнаружен zero‑delay цикл A→B→A. Либо поставь delay, либо переведи этот подграф в Core/Extended‑режим.»

Так ты получаешь:

* для тривиальных случаев — UX «как у обычной DAG‑библиотеки» (Airflow/Prefect‑style),
* при этом имеешь страховку от случайных циклов.

---

## 5. Интуитивный ответ на твой вопрос

> **Насколько меня это будет ограничивать как разработчика?**

* **Если твой use‑case — DAG без мгновенных feedback‑ов**: почти **никак**.
  Ты можешь абсолютно честно воспринимать фреймворк как «орchestrator с нодами‑функциями», и все разговоры про трёхзначную логику/constructive causality будут “под капотом” и не включатся.¹
* **Если ты залезаешь в реально хитрую реактивщину** (алгебраические циклы, мгновенные зависимости, сложные правила внутри одного тика),
  → именно там Core DSL **осознанно ограничивает** тебя, но взамен даёт формальные гарантии, которые иначе придётся держать только в голове и в тестах.²³

---

## 6. Термины (кратко)

* **Zero‑delay цикл / algebraic loop** — цикл зависимостей внутри одного логического тика (`x[t]` зависит от `x[t]` через другие сигналы); требует фикс‑пойнта/конструктивного анализа.²
* **DAG по мгновенным зависимостям** — граф, где нет таких циклов; его можно просто топологически отсортировать и выполнять без всякой трёхзначной математики.¹
* **Core DSL** — подъязык с AST‑выражениями и жёсткими ограничениями, для которого компилятор может доказать конструктивность.²³

---
